Subject: Technical Response - Coordinate Transformation, Rolling Shutter & Grip Slip Handling

Hi Brian,

Thank you for the detailed questions. These are exactly the right concerns for production-quality tracking. Here are my technical approaches:


1. COORDINATE TRANSFORMATION (Apple Vision → Unity)
---------------------------------------------------

I'm handling the handedness conversion through a structured transformation pipeline:

Position Transform:
- X remains unchanged (both systems use right-positive)
- Y remains unchanged (both use up-positive)  
- Z is negated (Apple: backward-positive, Unity: forward-positive)

Quaternion Transform:
For orientation, I negate the X and Y components while preserving W and Z. This maintains the rotation axis orientation while flipping the coordinate handedness.

The camera_extrinsics matrix from the Apple data is in column-major 4x4 format. I parse this into rotation and translation components, then apply the handedness flip to each before feeding into the kinematic chain. This ensures all my internal calculations happen in Unity-space coordinates, so the output is directly usable without runtime conversion.


2. ROLLING SHUTTER COMPENSATION AT 100mph
-----------------------------------------

At 100mph racket head velocity (~45 m/s) with iPhone's line-by-line sensor readout:
- At 30fps: ~20ms readout causes ~0.9m of racket movement across frame height
- At 120fps: ~6ms readout causes ~0.27m movement (much better)

My compensation approach:

For each detected joint, I calculate its temporal offset based on vertical pixel position:
   t_offset = (y_pixel / frame_height) × readout_time

Using the velocity estimate from my kinematic model, I back-project each joint to its actual position at the frame timestamp:
   p_corrected = p_detected - velocity × t_offset

The 120fps data you provided is significantly better for high-speed phases. I'm detecting motion speed per frame and only applying heavy compensation when speed > 10 m/s to avoid introducing noise during slow phases.


3. GRIP SLIP RECOVERY IN MINIMUM JERK
-------------------------------------

This is handled through residual monitoring and adaptive offset correction:

Detection:
I continuously compute the residual between predicted racket orientation (from kinematic chain) and any available measurement. A grip slip shows as:
- Sudden residual magnitude > 8° AND
- Residual rate of change > 500°/sec

Recovery:
When slip is detected, I:
1. Immediately apply an offset correction to maintain continuity
2. Terminate the current minimum jerk trajectory segment
3. Initialize a new trajectory segment from the corrected state
4. Apply exponential smoothing (α=0.3) over subsequent frames to refine the offset

Phase-aware thresholds:
During impact and acceleration phases (highest slip risk), I widen the detection threshold by 1.5-2x to avoid false positives from legitimate rapid rotation.


HYBRID MODEL PROPOSAL (Patent Potential)
----------------------------------------

I've been thinking about how to combine Apple Vision's ML predictions with our kinematic physics model in a way that could be genuinely novel:

The key insight: ML handles appearance variation well but violates physics; kinematics enforces physics but requires good initialization.

Proposed fusion architecture:
- Layer 1: Apple Vision raw detection
- Layer 2: Kinematic refinement (coordinate transform, anatomical constraints)
- Layer 3: Adaptive fusion with dynamic weighting based on confidence + speed
- Layer 4: Minimum jerk trajectory optimization

The adaptive weighting is the novel part:
- High confidence + low speed → trust ML more
- Low confidence + high speed → trust kinematics more
- During occlusion → full kinematic mode

This gives us the best of both approaches and creates a defensible technical contribution. I can document this formally if you'd like to explore patent filing.


IMPLEMENTATION STATUS
---------------------

I've updated the codebase to:
✓ Parse the new Apple Vision JSON format (30fps and 120fps)
✓ Apply coordinate transformation automatically
✓ Implement rolling shutter compensation
✓ Add grip slip detection and recovery
✓ Create hybrid fusion module

Tested successfully on vids1/30Fps-1 data. The tracking overlay is exported to _tracked.mp4 files.


Let me know if you need clarification on any of these approaches or want me to focus on specific aspects.

Best regards,
Prashun
Motion Architect

